#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ì €ì‚¬ì–‘ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ê°€ë²¼ìš´ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import json
import torch
import torch.nn as nn
from transformers import PreTrainedTokenizer, PreTrainedModel

class SimpleTokenizer:
    """ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € êµ¬í˜„"""
    
    def __init__(self):
        self.pad_token = "<pad>"
        self.eos_token = "</s>"
        self.pad_token_id = 0
        self.eos_token_id = 1
        self.vocab_size = 1000
        
        # ê°„ë‹¨í•œ ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘
        self.word2id = {self.pad_token: 0, self.eos_token: 1}
        self.id2word = {0: self.pad_token, 1: self.eos_token}
        
        # ê¸°ë³¸ í•œêµ­ì–´ ë‹¨ì–´ë“¤ ì¶”ê°€
        basic_words = [
            "API", "ìƒì„±", "ë°ì´í„°", "ì‘ë‹µ", "ìš”ì²­", "í•„ë“œ", "íƒ€ì…", "JSON",
            "OpenAPI", "ìŠ¤í™", "ê²½ë¡œ", "ë©”ì„œë“œ", "GET", "POST", "PUT", "DELETE",
            "ì„±ê³µ", "ì˜¤ë¥˜", "ê²€ì¦", "ì„¤ëª…", "ì˜ˆì œ", "ê°’", "ë¬¸ìì—´", "ìˆ«ì",
            "ë¶ˆë¦°", "ë°°ì—´", "ê°ì²´", "í•„ìˆ˜", "ì„ íƒ", "ê¸°ë³¸ê°’", "í˜•ì‹", "êµ¬ì¡°"
        ]
        
        for i, word in enumerate(basic_words, 2):
            self.word2id[word] = i
            self.id2word[i] = word
    
    def encode(self, text, **kwargs):
        """í…ìŠ¤íŠ¸ë¥¼ í† í° IDë¡œ ë³€í™˜"""
        words = text.split()
        ids = []
        for word in words:
            if word in self.word2id:
                ids.append(self.word2id[word])
            else:
                # ì•Œ ìˆ˜ ì—†ëŠ” ë‹¨ì–´ëŠ” í•´ì‹œ ê¸°ë°˜ìœ¼ë¡œ ID ìƒì„±
                hash_id = hash(word) % 100 + 100
                ids.append(hash_id)
        return ids
    
    def decode(self, ids, **kwargs):
        """í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        words = []
        for token_id in ids:
            if token_id in self.id2word:
                words.append(self.id2word[token_id])
            else:
                words.append(f"<unk_{token_id}>")
        return " ".join(words)
    
    def save_pretrained(self, path):
        """í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        # í† í¬ë‚˜ì´ì € ì„¤ì • ì €ì¥
        config = {
            "model_type": "simple",
            "pad_token": self.pad_token,
            "eos_token": self.eos_token,
            "vocab_size": self.vocab_size
        }
        
        with open(os.path.join(path, "tokenizer_config.json"), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # ì–´íœ˜ ì €ì¥
        vocab = {word: idx for word, idx in self.word2id.items()}
        with open(os.path.join(path, "vocab.json"), 'w', encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)

class SimpleModel(nn.Module):
    """ê°„ë‹¨í•œ ì–¸ì–´ ëª¨ë¸ êµ¬í˜„"""
    
    def __init__(self, vocab_size=1000, hidden_size=128, num_layers=2):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        
        # ì„ë² ë”© ë ˆì´ì–´
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        
        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)
        
        # ì¶œë ¥ ë ˆì´ì–´
        self.output = nn.Linear(hidden_size, vocab_size)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, input_ids, **kwargs):
        # ì„ë² ë”©
        embedded = self.embedding(input_ids)
        
        # LSTM ì²˜ë¦¬
        lstm_out, _ = self.lstm(embedded)
        
        # ë“œë¡­ì•„ì›ƒ ì ìš©
        lstm_out = self.dropout(lstm_out)
        
        # ì¶œë ¥
        logits = self.output(lstm_out)
        
        return logits
    
    def generate(self, input_ids, max_length=50, temperature=0.7, **kwargs):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        batch_size = input_ids.shape[0]
        current_ids = input_ids.clone()
        
        for _ in range(max_length - input_ids.shape[1]):
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = self.forward(current_ids)
                next_token_logits = outputs[:, -1, :] / temperature
                
                # ë‹¤ìŒ í† í° ì„ íƒ
                next_token = torch.multinomial(torch.softmax(next_token_logits, dim=-1), 1)
                
                # ID ì¶”ê°€
                current_ids = torch.cat([current_ids, next_token], dim=1)
                
                # EOS í† í°ì´ë©´ ì¤‘ë‹¨
                if (next_token == 1).any():  # EOS í† í° ID
                    break
        
        return current_ids
    
    def save_pretrained(self, path):
        """ëª¨ë¸ì„ ì €ì¥"""
        os.makedirs(path, exist_ok=True)
        
        # ëª¨ë¸ ì„¤ì • ì €ì¥
        config = {
            "model_type": "simple",
            "vocab_size": self.vocab_size,
            "hidden_size": self.hidden_size,
            "num_layers": 2
        }
        
        with open(os.path.join(path, "config.json"), 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
        torch.save(self.state_dict(), os.path.join(path, "pytorch_model.bin"))

def create_test_model():
    """í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ìƒì„±"""
    print("ğŸ§ª í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„± ì¤‘...")
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    model_dir = "/app/models"
    os.makedirs(model_dir, exist_ok=True)
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ ê²½ë¡œ
    test_model_path = os.path.join(model_dir, "test-model")
    
    try:
        # í† í¬ë‚˜ì´ì € ìƒì„±
        print("ğŸ“ í† í¬ë‚˜ì´ì € ìƒì„± ì¤‘...")
        tokenizer = SimpleTokenizer()
        tokenizer.save_pretrained(test_model_path)
        
        # ëª¨ë¸ ìƒì„±
        print("ğŸ¤– ëª¨ë¸ ìƒì„± ì¤‘...")
        model = SimpleModel()
        model.save_pretrained(test_model_path)
        
        # í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ìƒì„±
        env_file = os.path.join(model_dir, ".env")
        with open(env_file, 'w', encoding='utf-8') as f:
            f.write(f"MODEL_PATH={test_model_path}\n")
            f.write("SELECTED_MODEL=test-simple-model\n")
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ: {test_model_path}")
        print("ğŸ”§ ì´ì œ main.pyì—ì„œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    create_test_model() 